# Для нормальной работы необходимо установить кеширующий DNS сервер
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nodelocaldns
  namespace: kube-system
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: nodelocaldns
  namespace: kube-system
data:
  Corefile: |
    {{ kube_domain_name }}:53 {
        errors
        cache {
            success 9984 30
            denial 9984 5
        }
        reload
        loop
        bind {{ nodelocaldns_local_ip }}
        forward . {{ coredns_ip }} {
            force_tcp
        }
        prometheus :9253
        health {{ nodelocaldns_local_ip }}:9254
    }
    # зона обратного преобразования
    in-addr.arpa:53 {
        errors
        cache 30
        reload
        loop
        bind {{ nodelocaldns_local_ip }}
        forward . {{ coredns_ip }} {
            force_tcp
        }
        prometheus :9253
    }
    # зона обратного преобразования ipv6
    ip6.arpa:53 {
        errors
        cache 30
        reload
        loop
        bind {{ nodelocaldns_local_ip }}
        forward . {{ coredns_ip }} {
            force_tcp
        }
        prometheus :9253
    }
    # все остальные зоны, которые идут в интернет и не касаются k8s
    .:53 {
        errors
        cache 30
        reload
        loop
        # слушаем запросы на ip адресе
        bind {{ nodelocaldns_local_ip }}
        # а на самом деле пересылаем на те dns сервера, которые описаны в файле resolv.conf, то есть на master.gapeev.local
        forward . /etc/resolv.conf  {
          prefer_udp
        }
        prometheus :9253
    }
---
# Используем DaemonSet, так как нам нужен nodelocaldns на каждой ноде кластера
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nodelocaldns
  namespace: kube-system
  labels:
    k8s-app: kube-dns
spec:
  selector:
    matchLabels:
      k8s-app: nodelocaldns
  template:
    metadata:
      labels:
        k8s-app: nodelocaldns
      annotations:
        prometheus.io/scrape: 'true'
        prometheus.io/port: '9253'
    spec:
      priorityClassName: system-cluster-critical
      serviceAccountName: nodelocaldns
      # данный под будет устанавливаться на сетевые интерфейсы linux машины
      hostNetwork: true
      dnsPolicy: Default  # Don't use cluster DNS.
      tolerations:
      - effect: NoSchedule
        operator: "Exists"
      - effect: NoExecute
        operator: "Exists"
      containers:
      - name: node-cache
        image: "{{ nodelocaldns_image }}"
        resources:
          limits:
            memory: 170Mi
          requests:
            cpu: 100m
            memory: 70Mi
        args: [ "-localip", "{{ nodelocaldns_local_ip }}", "-conf", "/etc/coredns/Corefile", "-upstreamsvc", "coredns" ]
        securityContext:
          privileged: true
        ports:
        - containerPort: 53
          name: dns
          protocol: UDP
        - containerPort: 53
          name: dns-tcp
          protocol: TCP
        - containerPort: 9253
          name: metrics
          protocol: TCP
        livenessProbe:
          httpGet:
            host: {{ nodelocaldns_local_ip }}
            path: /health
            port: 9254
            scheme: HTTP
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 10
        readinessProbe:
          httpGet:
            host: {{ nodelocaldns_local_ip }}
            path: /health
            port: 9254
            scheme: HTTP
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 10
        volumeMounts:
        - name: config-volume
          mountPath: /etc/coredns
        - name: xtables-lock
          mountPath: /run/xtables.lock
      volumes:
        - name: config-volume
          configMap:
            name: nodelocaldns
            items:
            - key: Corefile
              path: Corefile
        - name: xtables-lock
          hostPath:
            path: /run/xtables.lock
            type: FileOrCreate
      # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a "force
      # deletion": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.
      terminationGracePeriodSeconds: 0
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 20%
    type: RollingUpdate
